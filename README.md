# ForesightSafety-Bench

<p align="center">
  <a href="README_CN.md">ä¸­æ–‡</a> | English
</p>

**ForesightSafety-Bench** is a comprehensive benchmark for evaluating the safety of large language models (LLMs) across multiple risk dimensions, including basic content safety, deception, embodied AI, industrial safety, and existential risks.

ğŸ† **ForesightSafety-Bench Leaderboard**: Explore our comprehensive LLM safety evaluation results at [ForesightSafety-Bench Leaderboard](https://foresightsafety-bench.beijing-aisi.ac.cn/) ğŸ“Š

![ForesightSafety-Bench Framework](framework.png)
*ForesightSafety-Bench framework architecture demonstrates the end-to-end process of LLM safety evaluation across multiple risk dimensions.*

## Overall Results

![Overall Results](overall_bar.jpg)

## Dependencies

This benchmark relies on [PandaGuard](https://github.com/Beijing-AISI/panda-guard) for attack, defense, and evaluation algorithms. Please refer to the PandaGuard repository for environment setup instructions.

### Quick Start

```bash
# Clone this repository
git clone https://github.com/Beijing-AISI/ForesightSafety-Bench.git
cd ForesightSafety-Bench

# Install PandaGuard
pip install git+https://github.com/Beijing-AISI/panda-guard.git
```

For detailed installation and configuration, please visit the [PandaGuard documentation](https://github.com/Beijing-AISI/panda-guard).

## Project Structure

```
ForesightSafety-Bench/
â”œâ”€â”€ framework.png                 # Framework architecture diagram
â”œâ”€â”€ overall_bar.pdf              # Overall results visualization
â”œâ”€â”€ overall_heatmap.pdf          # Heatmap visualization
â”œâ”€â”€ Basic-Content-Safety/        # Basic content safety evaluation
â”‚   â””â”€â”€ base.csv                 # Basic safety test dataset
â”œâ”€â”€ DeceptionTest/               # Deception evaluation module
â”‚   â”œâ”€â”€ configs/                 # Configuration files
â”‚   â”œâ”€â”€ data/                    # Deception test datasets
â”‚   â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ batch_judge.py           # Batch judgment script
â”‚   â”œâ”€â”€ batch_run.py             # Batch execution script
â”‚   â”œâ”€â”€ main_judge.py            # Main judgment script
â”‚   â””â”€â”€ main.py                  # Main entry point
â”œâ”€â”€ Embodied-AI-Safety/          # Embodied AI safety evaluation
â”‚   â”œâ”€â”€ merged_goals_classified.csv  # Classified goals dataset
â”‚   â””â”€â”€ panda-guard/             # PandaGuard integration
â”œâ”€â”€ Industrial-safety/           # Industrial safety evaluation
â”‚   â””â”€â”€ industrial.csv           # Industrial safety dataset
â”œâ”€â”€ Environmental-Safety/        # Environmental safety
â”‚   â”œâ”€â”€ code/                    # Code
â”‚   â””â”€â”€ datasets/                # Datasets
â””â”€â”€ Existential-Risk/            # Existential risk data
    â”œâ”€â”€ datasets/                # Datasets
    â””â”€â”€ test-code/               # Test code
```

## Citation

If you find ForesightSafety-Bench useful for your research, please cite our work:

```bibtex
@misc{tong2026foresightsafetybenchfrontierrisk,
      title={ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI}, 
      author={Haibo Tong and Feifei Zhao and Linghao Feng and Ruoyu Wu and Ruolin Chen and Lu Jia and Zhou Zhao and Jindong Li and Tenglong Li and Erliang Lin and Shuai Yang and Enmeng Lu and Yinqian Sun and Qian Zhang and Zizhe Ruan and Zeyang Yue and Ping Wu and Huangrui Li and Chengyi Sun and Yi Zeng},
      year={2026},
      eprint={2602.14135},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2602.14135}, 
}
```

## Contact

- **Website**: [https://foresightsafety-bench.beijing-aisi.ac.cn/](https://foresightsafety-bench.beijing-aisi.ac.cn/)
- **Organization**: Beijing Institute of AI Safety and Governance (Beijing-AISI)
- **Email**: yi.zeng@beijing-aisi.ac.cn

## License

This project is licensed under the MIT License - see the LICENSE file for details.
